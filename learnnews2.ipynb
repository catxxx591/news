{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import html5lib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cna.com.tw/list/aspt.aspx'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html5lib')\n",
    "n=0\n",
    "labels=[]\n",
    "while n < len(soup.select_one('#pnProductNavContents > ul').findAll('li')):\n",
    "    try :\n",
    "        lab = soup.select_one('#pnProductNavContents > ul').findAll('li')[n].find('a',{'class':'first-level'}).text\n",
    "        url =soup.select_one('#pnProductNavContents > ul').findAll('li')[n].find('a',{'class':'first-level'})['href']\n",
    "    except:(TypeError,IndentationError)\n",
    "    labels.append((url,lab))\n",
    "    n += 1\n",
    "labels = list(set(labels))\n",
    "labels.remove(('http://cnavideo.cna.com.tw', '影音'))\n",
    "labels.remove(('/list/aall.aspx', '即時'))\n",
    "labels.append(('https://www.cna.com.tw/list/aall.aspx', '即時'))\n",
    "#labels.remove(('https://www.cna.com.tw/keyword/九合一.aspx', '九合一'))\n",
    "labels.remove(('https://www.cna.com.tw/list/sp.aspx', '專題'))\n",
    "labels.remove(('https://www.cna.com.tw/list/aall.aspx', '即時'))\n",
    "\n",
    "labs = []\n",
    "for item in labels:\n",
    "    item = [item[0],item[1]]\n",
    "\n",
    "    item[0] = re.search(r'list\\/\\w+',item[0]).group()[5:]\n",
    "    labs.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payload(item):\n",
    "    payload={\n",
    "        'id': '1056893641102242',\n",
    "    'ev': 'Microdata',\n",
    "    'dl': item[0],\n",
    "    'rl': 'https://www.cna.com.tw/',\n",
    "    'if': 'false',\n",
    "    'ts': '1541087113735',\n",
    "    'cd[Schema.org]': [],\n",
    "    #'cd[OpenGraph]': {\"og:url\":\"https://www.cna.com.tw/list/ahel.aspx\",\"og:title\":\"生活 | 中央社 CNA\",\"og:description\":\"想知道統一發票、大樂透、威力彩開獎號碼、各種醫療保健、交通氣象、颱風動態等生活資訊，及升學考試、教育政策、課綱改革等文教訊息，通通都在中央社生活新聞。\",\"og:image:height\":\"200\",\"og:image:width\":\"200\",\"og:image\":\"https://img5.cna.com.tw/www/images/pic_fb.jpg\",\"og:type\":\"article\",\"article:author\":\"https://www.facebook.com/cnanewstaiwan\",\"article:publisher\":\"https://www.facebook.com/cnanewstaiwan\"}\n",
    "    #'cd[Meta]': {\"title\":\"\\n\\t生活 | 中央社 CNA\\n\",\"meta:description\":\"想知道統一發票、大樂透、威力彩開獎號碼、各種醫療保健、交通氣象、颱風動態等生活資訊，及升學考試、教育政策、課綱改革等文教訊息，通通都在中央社生活新聞。\",\"meta:keywords\":\"News, 新聞, 即時新聞, 中央社\"}\n",
    "    'cd[DataLayer]': [],\n",
    "    'cd[JSON-LD]': [],\n",
    "    'sw': '1280',\n",
    "    'sh': '5000',#'720',\n",
    "    'v': '2.8.30',\n",
    "    'r': 'stable',\n",
    "    'a': 'tmgoogletagmanager',\n",
    "    'ec': '1',\n",
    "    'o': '30',\n",
    "    'fbp': 'fb.2.1541074934894.620990608',\n",
    "    'it': '1541087112886',\n",
    "    'coo': 'false',\n",
    "    'es': 'automatic',\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_href=[]\n",
    "for item in labs:\n",
    "    soups=[]\n",
    "    n=1\n",
    "    while n < 13:\n",
    "        url = 'https://www.cna.com.tw/cna2018api/api/simplelist/categorycode/'+item[0]+'/pageidx/'+str(n)\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text,'html5lib')\n",
    "        body = soup.findAll('body')[0].getText()\n",
    "        PageUrl = re.findall(r'\\\"PageUrl\\\"\\:\\S[^\\,]+',body)\n",
    "        urls = [txt[11:-1] for txt in PageUrl]\n",
    "        soups.extend(urls)\n",
    "        n += 1\n",
    "        if len(urls) < 20:\n",
    "            break\n",
    "    labs_href.append([soups,item[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('產經', 20),('產經', 40),('產經', 60),('產經', 80),('產經', 100),('政治', 20),('政治', 40),('政治', 60),('政治', 80),('政治', 100),('地方', 20),('地方', 40),('地方', 60),('地方', 80),('地方', 100),('證券', 20),('證券', 40),('證券', 60),('證券', 80),('證券', 100),('娛樂', 20),('娛樂', 40),('娛樂', 60),('娛樂', 80),('娛樂', 100),('國際', 20),('國際', 40),('國際', 60),('國際', 80),('國際', 100),('兩岸', 20),('兩岸', 40),中央社駐上海特派員陳家倫/1月10日現年92歲的上海海關大鐘近來開始誤點，去年8月還一度中暑暈厥了好幾小時。衰老的大鐘預計於今年接受英國原廠「健檢」，不論結果是搶救或無痛地走向終點，都將轟動上海灘。上海海關大樓是外灘知名歷史建築，頂部的大鐘與倫敦大笨鐘是「姊妹」，由英國同一家公司生產。據稱，上海海關大鐘當年總造價高達5000多兩白銀，於1927年8月從倫敦運到上海，原包裝木箱連同大鐘重達6.25噸。1928年元旦凌晨1時，海關大鐘敲響了第一聲，從此以後，鐘聲就日以繼夜地陪伴著上海人。對於早年生活並不富裕的中國民眾而言，別說手上戴個表，連在家掛個時鐘都是奢侈，噹噹響的鐘聲因而成了市民珍視的「公共財」。「以前再遠都聽得到呦」，62歲的上海徐姓民眾從小在靜安寺附近長大，住家距離外灘約5公里遠。他回憶，學齡前就對海關大鐘的鐘聲有印象，但受到都市化的高樓阻絕、汽車雜音干擾，老家現已聽不到鐘聲。至於原先住在外灘一帶的居民，因都市計畫與拆遷的關係，也從海關大鐘的「搖滾區」撤出，接替而上的是大批境外觀光客，連帶將海關大鐘推向世界，鐘聲敲進了更多人的耳裡，並迴盪在老上海的記憶中。享受世人矚目的同時，邁入92歲的海關大鐘也面臨著歲月的考驗，自去年起，就開始不太準時，常有熱心民眾去電通報「大鐘時間錯了」；上海的夏季高溫則是另一道試煉，去年8月一個攝氏40度烤曬的午後，老鐘昏了過去。守鐘人魏云寺表示，海關大鐘自去年暑假開始，每到下午就走走停停，他當時就想，位於10樓的鐘樓溫度往往比外頭多出10至20度左右，溫差導致讓齒輪、零件走位，「診斷」結果，果然是擒縱器出了問題。魏云寺想起28年前自老前輩手中接下了一個英國原廠的擒縱器，立刻拿來給老鐘換上，10分鐘後，按下大鐘啟動開關，發條帶動135公斤的大方錘再度響起，鐘聲強而有力，魏云寺一聽這「心跳」，便知道老鐘痊癒了。更換零件的問題容易解決，但城市地景變動也影響了海關大鐘基座的平衡，大家都知道老鐘「健檢」已不能再拖。得知當時建造大鐘的英國廠家仍在經營，上海海關聯繫後，計畫於今年對大鐘進行整體評估。「健檢」時間目前暫定為農曆年過後。陸媒上觀新聞指出，屆時將由英國原廠提出最終方案，看老鐘是要大修還是壽終正寢。但為了保險起見，上海方面已對大鐘研究出一套馬達驅動、電子控制的方案，且已於2018年9月試運轉。據了解，這套方案完全替代了機械報時，整點鐘聲和報刻樂曲「東方紅」全部為電子播放。但為了保留歷史特色，目前整點鐘聲仍由老鐘重錘敲出，而每15分鐘的報刻音樂「東方紅」則早已是電子播放。提及海關大鐘的鐘聲，也有一段從西方走向本土的歷程。大鐘出廠時的音樂原本和英國大笨鐘一樣都是「西敏寺鐘聲」，也是外界一般熟知的下課鐘聲，後來改成「東方紅」，據稱是文化大革命為了「自保」。中國海關總署網站指出，1966年5月，一批「紅衛兵小將」闖入海關大樓，揚言砸毀鐘樓。「欣幸有一些頭腦清醒的海關幹部勇敢地站出來制止這些過激行為」。上海市政府並將報刻鐘聲改成「東方紅」，大鐘才躲過文革摧殘。由於中國許多景點都有一套相似的故事，真實性有待考據。但以「紅歌」做為鐘聲是否合適，過去有不少討論，上海並一度於1986年、英國女皇伊莉莎白二世造訪時改回「西敏寺鐘聲」，只是在引來「帝國主義」的質疑後又改回來。然而相關討論並未就此劃下句點，作為中國最開放的城市，上海一直有在思考要將鐘聲換回「西敏寺鐘聲」與國際接軌。改或不改，答案或有機會隨著老鐘「健檢」後一併出爐。揭曉的時間，已經開始倒數。（編輯：張淑伶）1080110\n",
      "('兩岸', 60),('兩岸', 80),('兩岸', 100),('生活', 20),('生活', 40),('生活', 60),('生活', 80),('生活', 100),('運動', 20),('運動', 40),('運動', 60),('運動', 80),('運動', 100),('科技', 20),('科技', 40),('科技', 60),('科技', 80),('科技', 100),('社會', 20),('社會', 40),('社會', 60),('社會', 80),('社會', 100),('文化', 20),('文化', 40),('文化', 60),('文化', 80),('文化', 100),"
     ]
    }
   ],
   "source": [
    "news={}\n",
    "newslist=[]\n",
    "for lab in labs_href:\n",
    "    lab_name = lab[1]\n",
    "    lab_urls = lab[0]\n",
    "    n = 0\n",
    "    for url in lab_urls:\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text,'html5lib')\n",
    "        txt = \"\"\n",
    "        try:\n",
    "            title = soup.find_all('div',{'class':'centralContent'})[0].find_all('h1')[0].getText()\n",
    "            for s in soup.find_all('div',{'class':'paragraph'})[0].findAll('p'):\n",
    "                txt=txt+s.getText()\n",
    "        except:IndexError\n",
    "        research = re.search(r'[\\）\\)\\：\\:\\》](.+[\\。\\!\\?\\s])+',txt)\n",
    "        if research == None or len(research.group())<2:\n",
    "            print(txt)\n",
    "        else:\n",
    "            txt = re.search(r'[\\）\\)\\：\\:\\》](.+[\\。\\!\\?\\s])+',txt).group()[1:]\n",
    "\n",
    "        page = [[title,txt],lab_name]\n",
    "        newslist.append(page)\n",
    "        news[(title,txt)]=lab_name\n",
    "        n += 1\n",
    "        if n % 20 == 0:\n",
    "            print((lab_name,n),end=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "with open('../30/txt/stopwords_ch.txt', 'r', encoding='UTF-8') as file:\n",
    "    for data in file.readlines():\n",
    "        data = data.strip()\n",
    "        stopwords.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tags ={}\n",
    "prelab = ''\n",
    "books = list(news.items())\n",
    "for book in books:\n",
    "    for lab in ['生活','運動','政治','科技','國際','兩岸','產經','社會','地方','娛樂','證券','文化']:\n",
    "        if book[1]==lab:\n",
    "            if prelab != lab:\n",
    "                txt=''\n",
    "            txt = txt+book[0][1]\n",
    "            lab_tags[book[1]] = txt\n",
    "            prelab = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterwords=[\n",
    "    '年','月','中央社','記者','編輯','譯者','日電'\n",
    "    ,'\\n',' ','「','」','『','』'\n",
    "    ,'今天','說','表示','年','月','日','後','前','時','應','中','今年'\n",
    "    ,'兩','下','人','最','次','再','先','盡量'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for news in newslist:\n",
    "    articles.append((news[0][1],news[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "\n",
    "\n",
    "for n in range(len(articles)):\n",
    "\n",
    "    txt_dic = {}\n",
    "    txt = articles[n]\n",
    "    txt_dic[txt[0]] = txt[1]\n",
    "    books.append(txt_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(books=books):\n",
    "    tags = {}  \n",
    "    labs = list(set([v for b in books for v in b.values()]))\n",
    "    for lab in labs:\n",
    "        txt = \"\"\n",
    "        for book in books:\n",
    "            if list(book.values())[0] == lab:\n",
    "                txt += list(book.keys())[0]\n",
    "        tags[lab] = txt\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tags = get_tags(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "allrawword =\"\"\n",
    "for v in txt_tags.values():\n",
    "    allrawword += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(txt=\"\"):\n",
    "    seg_list = jieba.cut(txt, cut_all=False)\n",
    "    txt = \",\".join(seg_list)\n",
    "    tokens = txt.split(',')\n",
    "    tokens = [re.match(r'\\w+',w).group() for w in tokens if (w not in stopwords and w not in filterwords) and re.match(r'\\w+',w) != None]\n",
    "    lab_alltwowords = list(nltk.bigrams(tokens))\n",
    "    tokens.extend([str(i[0])+str(i[1]) for i in lab_alltwowords])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fwords(txt,top=None):\n",
    "    tokens = get_tokens(txt)\n",
    "    return Counter(tokens).most_common(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fwords_tags(top=None):\n",
    "    \n",
    "    fwords_tags = {}\n",
    "    for i in list(txt_tags.items()):\n",
    "        fwords_tags[i[0]]=get_fwords(i[1],top)\n",
    "    return fwords_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwords_tags_items = get_fwords_tags(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('中國', 397),\n",
       " ('台灣', 188),\n",
       " ('香港', 186),\n",
       " ('北京', 152),\n",
       " ('報導', 131),\n",
       " ('國家', 115),\n",
       " ('大陸', 96),\n",
       " ('指出', 86),\n",
       " ('一國兩制', 84),\n",
       " ('共識', 83),\n",
       " ('已', 81),\n",
       " ('政府', 79),\n",
       " ('元', 75),\n",
       " ('2018', 74),\n",
       " ('習近平', 73),\n",
       " ('經濟', 73),\n",
       " ('去年', 69),\n",
       " ('中共', 69),\n",
       " ('兩岸', 65),\n",
       " ('九二', 64),\n",
       " ('工作', 64),\n",
       " ('九二共識', 64),\n",
       " ('市場', 63),\n",
       " ('名', 62),\n",
       " ('上海', 60),\n",
       " ('提出', 58),\n",
       " ('12', 56),\n",
       " ('認為', 55),\n",
       " ('民主', 54),\n",
       " ('新', 53),\n",
       " ('進行', 53),\n",
       " ('美國', 52),\n",
       " ('總統', 52),\n",
       " ('問題', 51),\n",
       " ('會議', 50),\n",
       " ('中國大陸', 50),\n",
       " ('方面', 49),\n",
       " ('發展', 49),\n",
       " ('中央', 48),\n",
       " ('公司', 48),\n",
       " ('相關', 47),\n",
       " ('人員', 47),\n",
       " ('城市', 45),\n",
       " ('關係', 44),\n",
       " ('10', 44),\n",
       " ('接受', 43),\n",
       " ('大學', 42),\n",
       " ('項', 41),\n",
       " ('政策', 41),\n",
       " ('委員會', 41),\n",
       " ('發表', 40),\n",
       " ('中心', 40),\n",
       " ('台', 40),\n",
       " ('包括', 40),\n",
       " ('統一', 40),\n",
       " ('發布', 40),\n",
       " ('聯合', 40),\n",
       " ('民眾', 39),\n",
       " ('社會', 39),\n",
       " ('未', 38),\n",
       " ('新聞', 38),\n",
       " ('媒體', 38),\n",
       " ('要求', 37),\n",
       " ('國際', 36),\n",
       " ('當局', 36),\n",
       " ('管理', 36),\n",
       " ('政治', 35),\n",
       " ('蔡', 34),\n",
       " ('目前', 34),\n",
       " ('曾', 34),\n",
       " ('蔡總統', 34),\n",
       " ('自由', 33),\n",
       " ('人士', 33),\n",
       " ('服務', 33),\n",
       " ('北京方面', 33),\n",
       " ('支持', 32),\n",
       " ('11', 32),\n",
       " ('機構', 32),\n",
       " ('企業', 32),\n",
       " ('內容', 32),\n",
       " ('規定', 32),\n",
       " ('安全', 31),\n",
       " ('官方', 31),\n",
       " ('情況', 31),\n",
       " ('投資', 31),\n",
       " ('種', 31),\n",
       " ('行政', 31),\n",
       " ('談話', 30),\n",
       " ('些', 30),\n",
       " ('2019', 30),\n",
       " ('教師', 30),\n",
       " ('回應', 29),\n",
       " ('報告', 29),\n",
       " ('億', 29),\n",
       " ('泛民', 29),\n",
       " ('選舉', 28),\n",
       " ('文章', 28),\n",
       " ('顯示', 28),\n",
       " ('稱', 28),\n",
       " ('旅遊', 28),\n",
       " ('活動', 28),\n",
       " ('平台', 28),\n",
       " ('立法會', 28),\n",
       " ('訪問', 27),\n",
       " ('改革', 27),\n",
       " ('高', 27),\n",
       " ('教育', 27),\n",
       " ('學生', 27),\n",
       " ('實施', 27),\n",
       " ('人口', 27),\n",
       " ('陸委會', 27),\n",
       " ('提到', 26),\n",
       " ('成長', 26),\n",
       " ('部門', 26),\n",
       " ('制度', 26),\n",
       " ('運動', 26),\n",
       " ('學校', 25),\n",
       " ('希望', 25),\n",
       " ('約', 25),\n",
       " ('主要', 24),\n",
       " ('強調', 24),\n",
       " ('意見', 24),\n",
       " ('代表', 24),\n",
       " ('40', 24),\n",
       " ('中美', 24),\n",
       " ('已經', 24),\n",
       " ('調查', 24),\n",
       " ('共同', 23),\n",
       " ('萬', 23),\n",
       " ('堅持', 23),\n",
       " ('達到', 23),\n",
       " ('全國', 23),\n",
       " ('15', 23),\n",
       " ('條', 23),\n",
       " ('處理', 23),\n",
       " ('警方', 23),\n",
       " ('屆', 23),\n",
       " ('資訊', 23),\n",
       " ('主席', 22),\n",
       " ('新疆', 22),\n",
       " ('過去', 22),\n",
       " ('透過', 22),\n",
       " ('影響', 22),\n",
       " ('措施', 22),\n",
       " ('指', 22),\n",
       " ('遭', 22),\n",
       " ('成為', 22),\n",
       " ('機關', 22),\n",
       " ('超過', 22),\n",
       " ('進一步', 22),\n",
       " ('發生', 22),\n",
       " ('舉行', 22),\n",
       " ('市', 22),\n",
       " ('汽車', 22),\n",
       " ('資格', 22),\n",
       " ('億元', 22),\n",
       " ('引述', 21),\n",
       " ('主權', 21),\n",
       " ('建設', 21),\n",
       " ('消費', 21),\n",
       " ('歷史', 21),\n",
       " ('價格', 21),\n",
       " ('昨天', 21),\n",
       " ('網路', 21),\n",
       " ('2017', 21),\n",
       " ('特別', 21),\n",
       " ('重要', 21),\n",
       " ('普選', 21),\n",
       " ('行為', 21),\n",
       " ('威脅', 20),\n",
       " ('主張', 20),\n",
       " ('地方', 20),\n",
       " ('金融', 20),\n",
       " ('目標', 20),\n",
       " ('網民', 20),\n",
       " ('出現', 20),\n",
       " ('公布', 20),\n",
       " ('人民幣', 20),\n",
       " ('申請', 20),\n",
       " ('針對', 20),\n",
       " ('居民', 20),\n",
       " ('資本', 20),\n",
       " ('領導', 20),\n",
       " ('決定', 20),\n",
       " ('北京市', 20),\n",
       " ('召開', 20),\n",
       " ('上述', 20),\n",
       " ('基本法', 20),\n",
       " ('議員', 20),\n",
       " ('雙方', 20),\n",
       " ('作戰', 20),\n",
       " ('國歌', 20),\n",
       " ('中國國家', 20),\n",
       " ('日前', 19),\n",
       " ('做', 19),\n",
       " ('期間', 19),\n",
       " ('地區', 19),\n",
       " ('首', 19),\n",
       " ('事件', 19),\n",
       " ('家', 19),\n",
       " ('20', 19),\n",
       " ('青年', 19),\n",
       " ('時間', 19),\n",
       " ('英國', 19),\n",
       " ('陳', 19),\n",
       " ('推特', 19),\n",
       " ('大鐘', 19),\n",
       " ('分析', 18),\n",
       " ('尊重', 18),\n",
       " ('宣布', 18),\n",
       " ('增加', 18),\n",
       " ('全球', 18),\n",
       " ('行動', 18),\n",
       " ('數據', 18),\n",
       " ('近日', 18),\n",
       " ('縣', 18),\n",
       " ('就業', 18),\n",
       " ('集團', 18),\n",
       " ('19', 18),\n",
       " ('航空', 18),\n",
       " ('參加', 18),\n",
       " ('以上', 18),\n",
       " ('規模', 18),\n",
       " ('同意', 18),\n",
       " ('副', 18),\n",
       " ('位', 18),\n",
       " ('6', 18),\n",
       " ('許多', 18),\n",
       " ('伊斯蘭教', 18),\n",
       " ('資本市場', 18),\n",
       " ('方案', 17),\n",
       " ('未來', 17),\n",
       " ('官員', 17),\n",
       " ('週年', 17),\n",
       " ('令', 17),\n",
       " ('14', 17),\n",
       " ('1', 17),\n",
       " ('下降', 17),\n",
       " ('方式', 17),\n",
       " ('原則', 17),\n",
       " ('提供', 17),\n",
       " ('正式', 17),\n",
       " ('法院', 17),\n",
       " ('副中心', 17),\n",
       " ('立場', 16),\n",
       " ('國務院', 16),\n",
       " ('一直', 16),\n",
       " ('加強', 16),\n",
       " ('持續', 16),\n",
       " ('擴大', 16),\n",
       " ('交流', 16),\n",
       " ('當時', 16),\n",
       " ('關注', 16),\n",
       " ('協會', 16),\n",
       " ('退學', 16),\n",
       " ('無', 16),\n",
       " ('政協', 16),\n",
       " ('市委', 16),\n",
       " ('規劃', 16),\n",
       " ('iPhone', 16),\n",
       " ('小組', 16),\n",
       " ('作出', 16),\n",
       " ('產生', 16),\n",
       " ('開放', 16),\n",
       " ('公開', 16),\n",
       " ('調整', 16),\n",
       " ('占', 16),\n",
       " ('和平', 16),\n",
       " ('疫苗', 16),\n",
       " ('酒店', 16),\n",
       " ('鐘聲', 16),\n",
       " ('經濟成長', 16),\n",
       " ('計畫', 15),\n",
       " ('國民黨', 15),\n",
       " ('空間', 15),\n",
       " ('穩定', 15),\n",
       " ('支出', 15),\n",
       " ('以來', 15),\n",
       " ('現在', 15),\n",
       " ('至今', 15),\n",
       " ('低', 15),\n",
       " ('官網', 15),\n",
       " ('合作', 15),\n",
       " ('實現', 15),\n",
       " ('新台幣', 15),\n",
       " ('50', 15),\n",
       " ('利益', 15),\n",
       " ('央視', 15),\n",
       " ('事', 15),\n",
       " ('鏈', 15),\n",
       " ('展振振', 15),\n",
       " ('2014', 15),\n",
       " ('2016', 15),\n",
       " ('港人', 15),\n",
       " ('港澳', 15),\n",
       " ('中小學', 15),\n",
       " ('貧困戶', 15),\n",
       " ('特首', 15),\n",
       " ('海關', 15),\n",
       " ('演藝', 15)]"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwords_tags_items['兩岸']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = [i[0] for i in get_fwords(allrawword,300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['台灣',\n",
       " '元',\n",
       " '中國',\n",
       " '去年',\n",
       " '已',\n",
       " '美國',\n",
       " '億',\n",
       " '指出',\n",
       " '新',\n",
       " '政府',\n",
       " '文化',\n",
       " '12',\n",
       " '億元',\n",
       " '10',\n",
       " '分',\n",
       " '國家',\n",
       " '公司',\n",
       " '名',\n",
       " '報導',\n",
       " '總統',\n",
       " '希望',\n",
       " '2018',\n",
       " '進行',\n",
       " '包括',\n",
       " '市場',\n",
       " '目前',\n",
       " '營收',\n",
       " '曾',\n",
       " '約',\n",
       " '中心',\n",
       " '11',\n",
       " '發展',\n",
       " '新台幣',\n",
       " '未來',\n",
       " '計畫',\n",
       " '國際',\n",
       " '認為',\n",
       " '相關',\n",
       " '民眾',\n",
       " '歲',\n",
       " '萬',\n",
       " '透過',\n",
       " '工作',\n",
       " '經濟',\n",
       " '項',\n",
       " '蘇貞昌',\n",
       " '2019',\n",
       " '技術',\n",
       " '問題',\n",
       " '高',\n",
       " '活動',\n",
       " '人員',\n",
       " '點',\n",
       " '成長',\n",
       " '藝術',\n",
       " '台北',\n",
       " '媒體',\n",
       " '影響',\n",
       " '香港',\n",
       " '持續',\n",
       " '時間',\n",
       " '產業',\n",
       " '調查',\n",
       " '大學',\n",
       " '提供',\n",
       " '服務',\n",
       " '歷史',\n",
       " '家',\n",
       " '做',\n",
       " '30',\n",
       " '科技',\n",
       " '演出',\n",
       " '蘋果',\n",
       " '全球',\n",
       " '種',\n",
       " '產品',\n",
       " '政策',\n",
       " '部分',\n",
       " '未',\n",
       " '宣布',\n",
       " '萬元',\n",
       " '日本',\n",
       " '合作',\n",
       " '季',\n",
       " '要求',\n",
       " '提出',\n",
       " '北京',\n",
       " '20',\n",
       " '使用',\n",
       " '陳',\n",
       " '好',\n",
       " '發現',\n",
       " '場',\n",
       " '舉辦',\n",
       " '表現',\n",
       " '社會',\n",
       " '警方',\n",
       " '電影',\n",
       " '舉行',\n",
       " '重要',\n",
       " '高雄',\n",
       " '首',\n",
       " '賴清德',\n",
       " '人士',\n",
       " '接受',\n",
       " '主要',\n",
       " '支持',\n",
       " '大陸',\n",
       " '遭',\n",
       " '委員會',\n",
       " '地方',\n",
       " '已經',\n",
       " '過去',\n",
       " '完成',\n",
       " '世界',\n",
       " '些',\n",
       " '內容',\n",
       " '手機',\n",
       " '機會',\n",
       " '發生',\n",
       " '部',\n",
       " '擔任',\n",
       " '推出',\n",
       " '投資',\n",
       " '處理',\n",
       " '中央',\n",
       " '代表',\n",
       " '天',\n",
       " '昨天',\n",
       " '是否',\n",
       " '立委',\n",
       " '選舉',\n",
       " '增加',\n",
       " '音樂',\n",
       " '0',\n",
       " '規定',\n",
       " '協助',\n",
       " '成為',\n",
       " '加拿大',\n",
       " '最後',\n",
       " '業者',\n",
       " '超過',\n",
       " '創作',\n",
       " '台中',\n",
       " '15',\n",
       " '團隊',\n",
       " '安全',\n",
       " '網路',\n",
       " '作品',\n",
       " '民進黨',\n",
       " '方式',\n",
       " '達',\n",
       " '公布',\n",
       " '執行',\n",
       " '內閣',\n",
       " '智慧',\n",
       " 'iPhone',\n",
       " '預計',\n",
       " '無',\n",
       " '蔡',\n",
       " '華為',\n",
       " '台',\n",
       " '下午',\n",
       " '決定',\n",
       " '16',\n",
       " '只',\n",
       " '共同',\n",
       " '最佳',\n",
       " '企業',\n",
       " '2017',\n",
       " '參與',\n",
       " '外',\n",
       " '推動',\n",
       " '13',\n",
       " '系統',\n",
       " '補助',\n",
       " '位',\n",
       " '英國',\n",
       " '去年12',\n",
       " '關係',\n",
       " '報告',\n",
       " '針對',\n",
       " '當時',\n",
       " '1',\n",
       " '需要',\n",
       " '卻',\n",
       " '特別',\n",
       " '行政院長',\n",
       " '行政院',\n",
       " '期間',\n",
       " '顯示',\n",
       " '14',\n",
       " '申請',\n",
       " '管理',\n",
       " '許多',\n",
       " '比賽',\n",
       " '教育',\n",
       " '地區',\n",
       " '政治',\n",
       " '新高',\n",
       " '集團',\n",
       " '小',\n",
       " '現場',\n",
       " '單位',\n",
       " '造成',\n",
       " '上午',\n",
       " '設計',\n",
       " '規劃',\n",
       " '研究',\n",
       " '立法院',\n",
       " '號',\n",
       " '共識',\n",
       " '需求',\n",
       " '蔡總統',\n",
       " '文化部',\n",
       " '發布',\n",
       " '需',\n",
       " '雙方',\n",
       " '發表',\n",
       " '年增',\n",
       " '這是',\n",
       " '行動',\n",
       " '進入',\n",
       " '應用',\n",
       " '川普',\n",
       " '日前',\n",
       " '市長',\n",
       " '男子',\n",
       " '現在',\n",
       " '想',\n",
       " '營運',\n",
       " '情況',\n",
       " '無法',\n",
       " '積極',\n",
       " '全國',\n",
       " '更多',\n",
       " '過程',\n",
       " '會議',\n",
       " '目標',\n",
       " '取得',\n",
       " '記者會',\n",
       " '繼續',\n",
       " '獲得',\n",
       " '資料',\n",
       " '環境',\n",
       " '習近平',\n",
       " '預算',\n",
       " '處',\n",
       " '以上',\n",
       " '紀錄',\n",
       " '強調',\n",
       " '年度',\n",
       " '方案',\n",
       " '國民黨',\n",
       " '臉書',\n",
       " '螢幕',\n",
       " '方面',\n",
       " '兩岸',\n",
       " '平台',\n",
       " '召開',\n",
       " '網站',\n",
       " '合併',\n",
       " '討論',\n",
       " '5G',\n",
       " '18',\n",
       " '一年',\n",
       " '城市',\n",
       " '學校',\n",
       " '屆',\n",
       " '局',\n",
       " '民主',\n",
       " '生活',\n",
       " '印度',\n",
       " '提升',\n",
       " '太空',\n",
       " '說明',\n",
       " '前往',\n",
       " '新北',\n",
       " '以來',\n",
       " '正式',\n",
       " '台北市',\n",
       " '減少',\n",
       " '張',\n",
       " '兩人',\n",
       " '拿下',\n",
       " '受',\n",
       " '時代',\n",
       " '法院',\n",
       " '傳統',\n",
       " '法人',\n",
       " '解決',\n",
       " '預期',\n",
       " '進一步',\n",
       " '參加',\n",
       " '電子',\n",
       " '球員',\n",
       " '一國兩制',\n",
       " '訪問',\n",
       " '主席',\n",
       " '40']"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mix(lab_fwords={}):\n",
    "\n",
    "    dic3={}\n",
    "    for lab in lab_fwords.keys():   \n",
    "        word_content2 = lab_fwords[lab].copy()\n",
    "#        if fword_value == \"dic\":\n",
    "#            word_content2 = list(word_content2.items())\n",
    "        for lab2 in lab_fwords.keys():    \n",
    "            if lab != lab2:\n",
    "#               if fword_value == \"items\":\n",
    "                word_content = [w for w in lab_fwords[lab] if (w[0] in [a[0] for a in lab_fwords[lab2]])]\n",
    "                for w in word_content:\n",
    "                    try:\n",
    "                        word_content2.remove(w)                  \n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                        \n",
    "                        \n",
    "#            if fword_value == \"items\":\n",
    "            dic3[lab] = word_content2 \n",
    "\n",
    "\n",
    "    return dic3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "fword_items = remove_mix(fwords_tags_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allfword = [i[0] for v in fword_items.values() for i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'www' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-344-76e4d9d534f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwww\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'www' is not defined"
     ]
    }
   ],
   "source": [
    "www"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_fwords(article,top=200):\n",
    "\n",
    "    features={}\n",
    "    fwords = get_fwords(article,top)\n",
    "    for w in fwords :\n",
    "        w = w[0]\n",
    "        if w in [i[0] for i in [t for v in fword_items.values() for t in v]]:\n",
    "            features[w] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dic={},top=200):\n",
    "\n",
    "    article = list(dic.keys())[0]\n",
    "    labname = list(dic.values())[0]\n",
    "    features = pos_fwords(article,top)\n",
    "\n",
    "    if len(list(features.keys())) < 2:\n",
    "        feature_item = [{},labname]\n",
    "    else:\n",
    "        feature_item = [features,labname]\n",
    "\n",
    "    return feature_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featuresets(books=books,top=200):\n",
    "    txt_featuresets = []\n",
    "    for book in books:\n",
    "\n",
    "        txt_featureset = get_features(book,top=top)\n",
    "        txt_featuresets.append(txt_featureset)\n",
    "    return  txt_featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-361-f07a4f355862>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtxt_featuresets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_featuresets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-360-f241f5bb3e81>\u001b[0m in \u001b[0;36mget_featuresets\u001b[1;34m(books, top)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mtxt_featureset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mtxt_featuresets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt_featureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m  \u001b[0mtxt_featuresets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-359-3d7842ffff82>\u001b[0m in \u001b[0;36mget_features\u001b[1;34m(dic, top)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlabname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_fwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-358-f932fd8b0c72>\u001b[0m in \u001b[0;36mpos_fwords\u001b[1;34m(article, top)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfwords\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfword_items\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-358-f932fd8b0c72>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfwords\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfword_items\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "txt_featuresets = get_featuresets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_featuresets = txt_featuresets.copy()\n",
    "size = int(len(txt_featuresets)*0.7)\n",
    "train_set, test_set = txt_featuresets[:size], txt_featuresets[size:]\n",
    "txt_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(txt_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[0]  for v in fwords_tags_items.values() for i in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwords_tags_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_top(article,top=5,get=20):\n",
    "    fwords = get_fwords(article)\n",
    "    txt_comanwer = txt_classifier.classify(pos_fwords(article))\n",
    "\n",
    "    labcount = []\n",
    "    for i in fwords:\n",
    "        w = i[0]\n",
    "        for lab in fwords_tags_items.keys():\n",
    "            if w in [wi[0] for wi in fwords_tags_items[lab]]:\n",
    "                labcount.append(lab)\n",
    "\n",
    "    commonlist = Counter(labcount).most_common()\n",
    "    \n",
    "#    if commonlist[0][1] - commonlist[1][1] <= 2 and txt_comanwer in [i[0]for i in commonlist[:3]]:\n",
    "#        most_common = txt_comanwer\n",
    "#    else :\n",
    "#        most_common = commonlist[0][0]\n",
    "#    print(commonlist,txt_comanwer,most_common)\n",
    "    return [i[1] for i in commonlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top(testtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(get_top(i[0]),i[1]) for i in [list(book.items())[0] for book in books[:30]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[list(book.keys())[0] for book in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "def getVocabularyList(content_list, vocabulary_size):\n",
    "    allContent_str = ''.join(content_list)\n",
    "    counter = Counter(allContent_str)\n",
    "    vocabulary_list = [k[0] for k in counter.most_common(vocabulary_size)]\n",
    "    return ['PAD'] + vocabulary_list\n",
    "startTime = time.time()\n",
    "vocabulary_list = getVocabularyList(content_list, 300)\n",
    "used_time = time.time() - startTime\n",
    "print('used time: %.2f seconds' %used_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = [i[0] for c in content_list for i in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict([(b, a) for a, b in enumerate(vocabulary_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idlist_list = [content2idList(content) for content in test_content_list]\n",
    "test_X = kr.preprocessing.sequence.pad_sequences(test_idlist_list, sequence_length)\n",
    "test_y = labelEncoder.transform(test_label_list)\n",
    "test_Y = kr.utils.to_categorical(test_y, num_classes)\n",
    "import random\n",
    "for i in range(10000):\n",
    "    selected_index = random.sample(list(range(len(train_y))), k=batch_size)\n",
    "    batch_X = train_X[selected_index]\n",
    "    batch_Y = train_Y[selected_index]\n",
    "    session.run(train, {X_holder:batch_X, Y_holder:batch_Y})\n",
    "    step = i + 1 \n",
    "    if step % 50 == 0:\n",
    "        selected_index = random.sample(list(range(len(test_y))), k=200)\n",
    "        batch_X = test_X[selected_index]\n",
    "        batch_Y = test_Y[selected_index]\n",
    "        loss_value, accuracy_value = session.run([loss, accuracy], {X_holder:batch_X, Y_holder:batch_Y})\n",
    "        print('step:%d loss:%.4f accuracy:%.4f' %(step, loss_value, accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idlist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def predictAll(test_X, batch_size=100):\n",
    "    predict_value_list = []\n",
    "    for i in range(0, len(test_X), batch_size):\n",
    "        selected_X = test_X[i: i + batch_size]\n",
    "        predict_value = session.run(predict_Y, {X_holder:selected_X})\n",
    "        predict_value_list.extend(predict_value)\n",
    "    return np.array(predict_value_list)\n",
    "\n",
    "Y = predictAll(test_X)\n",
    "y = np.argmax(Y, axis=1)\n",
    "predict_label_list = labelEncoder.inverse_transform(y)\n",
    "pd.DataFrame(confusion_matrix(test_label_list, predict_label_list), \n",
    "             columns=labelEncoder.classes_,\n",
    "             index=labelEncoder.classes_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwords_tags_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtxt = list(books[0].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_mix(get_fwords(testtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featureword(txt):\n",
    "    fw = [i for i in get_fwords(txt) if i[0] in [i[0] for v in fwords_tags_items.values() for i in v]]\n",
    "    return fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_featureword(testtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(books)\n",
    "content_list = [list(book.keys())[0] for book in books]\n",
    "label_list = [list(book.values())[0] for book in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(content_list, label_list)\n",
    "train_content_list = train_X\n",
    "train_label_list = train_y\n",
    "test_content_list = test_X\n",
    "test_label_list = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_test_split used time : 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(content_list, label_list)\n",
    "train_content_list = train_X\n",
    "train_label_list = train_y\n",
    "test_content_list = test_X\n",
    "test_label_list = test_y\n",
    "used_time = time.time() - startTime\n",
    "print('train_test_split used time : %.2f seconds' %used_time)\n",
    "#vocabulary_size = len(vocabulary_list)  # 词汇表达小\n",
    "vocabulary_size = len(allfword)\n",
    "\n",
    "sequence_length = 200#600  # 序列长度\n",
    "embedding_size = 64  # 词向量维度\n",
    "num_filters = 256  # 卷积核数目\n",
    "filter_size = 5  # 卷积核尺寸\n",
    "num_fc_units = 128  # 全连接层神经元\n",
    "dropout_keep_probability = 0.5  # dropout保留比例\n",
    "learning_rate = 1e-3  # 学习率\n",
    "batch_size = 40#64  # 每批训练大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allfwords = [i[0] for v in fwords_tags_items.values() for i in v]\n",
    "#Counter(allfword).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary_list = [i[0] for i in get_fwords(allrawword,4000)]\n",
    "vocabulary_list = [i for i in Counter(allfword).keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一國兩制', '中共', '上海', '方面', '統一', '聯合', '北京方面', '官方', '談話', '泛民']"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id_dict = dict([(d,idx+1) for idx,d in enumerate(vocabulary_list)])\n",
    "#word2id_dict = dict([(d,idx+1) for idx,d in enumerate(allfword)])\n",
    "\n",
    "word2id_dict[\"\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "content2idList = lambda content : [word2id_dict[word] for word in content if word in word2id_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content2idList used time : 0.78 seconds\n",
      "data preparation used time : 0.80 seconds\n"
     ]
    }
   ],
   "source": [
    "train_idlist_list = [content2idList(content) for content in train_content_list]\n",
    "used_time = time.time() - startTime\n",
    "print('content2idList used time : %.2f seconds' %used_time)\n",
    "import numpy as np\n",
    "num_classes = np.unique(label_list).shape[0]\n",
    "import tensorflow.contrib.keras as kr\n",
    "train_X = kr.preprocessing.sequence.pad_sequences(train_idlist_list, sequence_length)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder = LabelEncoder()\n",
    "train_y = labelEncoder.fit_transform(train_label_list)\n",
    "train_Y = kr.utils.to_categorical(train_y, num_classes)\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "X_holder = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y_holder = tf.placeholder(tf.float32, [None, num_classes])\n",
    "used_time = time.time() - startTime\n",
    "print('data preparation used time : %.2f seconds' %used_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.get_variable('embedding', \n",
    "                            [vocabulary_size, embedding_size])\n",
    "embedding_inputs = tf.nn.embedding_lookup(embedding,\n",
    "                                          X_holder)\n",
    "conv = tf.layers.conv1d(embedding_inputs,\n",
    "                        num_filters,\n",
    "                        filter_size)\n",
    "max_pooling = tf.reduce_max(conv, \n",
    "                            [1])\n",
    "full_connect = tf.layers.dense(max_pooling,\n",
    "                               num_fc_units)\n",
    "full_connect_dropout = tf.contrib.layers.dropout(full_connect, \n",
    "                                                 keep_prob=dropout_keep_probability)\n",
    "full_connect_activate = tf.nn.relu(full_connect_dropout)\n",
    "softmax_before = tf.layers.dense(full_connect_activate,\n",
    "                                 num_classes)\n",
    "predict_Y = tf.nn.softmax(softmax_before)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y_holder,\n",
    "                                                           logits=softmax_before)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "isCorrect = tf.equal(tf.argmax(Y_holder, 1), tf.argmax(predict_Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(isCorrect, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:100 loss:1.7996 accuracy:0.3900\n",
      "step:200 loss:1.5697 accuracy:0.4150\n",
      "step:300 loss:1.3952 accuracy:0.5950\n",
      "step:400 loss:1.6083 accuracy:0.5550\n",
      "step:500 loss:1.6454 accuracy:0.5400\n",
      "step:600 loss:1.6101 accuracy:0.6100\n",
      "step:700 loss:1.5938 accuracy:0.5900\n",
      "step:800 loss:2.0217 accuracy:0.5200\n"
     ]
    }
   ],
   "source": [
    "test_idlist_list = [content2idList(content) for content in test_content_list]\n",
    "test_X = kr.preprocessing.sequence.pad_sequences(test_idlist_list, sequence_length)\n",
    "test_y = labelEncoder.transform(test_label_list)\n",
    "test_Y = kr.utils.to_categorical(test_y, num_classes)\n",
    "import random\n",
    "for i in range(20000):\n",
    "    selected_index = random.sample(list(range(len(train_y))), k=batch_size)\n",
    "    batch_X = train_X[selected_index]\n",
    "    batch_Y = train_Y[selected_index]\n",
    "    session.run(train, {X_holder:batch_X, Y_holder:batch_Y})\n",
    "    step = i + 1 \n",
    "    if step % 100 == 0:\n",
    "        selected_index = random.sample(list(range(len(test_y))), k=200)\n",
    "        batch_X = test_X[selected_index]\n",
    "        batch_Y = test_Y[selected_index]\n",
    "        loss_value, accuracy_value = session.run([loss, accuracy], {X_holder:batch_X, Y_holder:batch_Y})\n",
    "        print('step:%d loss:%.4f accuracy:%.4f' %(step, loss_value, accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_dict = dict([(i[1],i[0]) for i in word2id_dict.items()])\n",
    "[id2word_dict[w] for w in test_X[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_dict[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X[40]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
