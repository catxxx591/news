{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新聞分類模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 爬取中央社新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import html5lib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.cna.com.tw/list/aspt.aspx'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html5lib')\n",
    "n=0\n",
    "labels=[]\n",
    "while n < len(soup.select_one('#pnProductNavContents > ul').findAll('li')):\n",
    "    try :\n",
    "        lab = soup.select_one('#pnProductNavContents > ul').findAll('li')[n].find('a',{'class':'first-level'}).text\n",
    "        url =soup.select_one('#pnProductNavContents > ul').findAll('li')[n].find('a',{'class':'first-level'})['href']\n",
    "    except:(TypeError,IndentationError)\n",
    "    labels.append((url,lab))\n",
    "    n += 1\n",
    "labels = list(set(labels))\n",
    "labels.remove(('http://cnavideo.cna.com.tw', '影音'))\n",
    "labels.remove(('/list/aall.aspx', '即時'))\n",
    "labels.append(('https://www.cna.com.tw/list/aall.aspx', '即時'))\n",
    "#labels.remove(('https://www.cna.com.tw/keyword/九合一.aspx', '九合一'))\n",
    "labels.remove(('https://www.cna.com.tw/list/sp.aspx', '專題'))\n",
    "labels.remove(('https://www.cna.com.tw/list/aall.aspx', '即時'))\n",
    "\n",
    "labs = []\n",
    "for item in labels:\n",
    "    item = [item[0],item[1]]\n",
    "\n",
    "    item[0] = re.search(r'list\\/\\w+',item[0]).group()[5:]\n",
    "    labs.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用payload()post中央社網站分類底下各標題的href連結，整理成labs_href\n",
    "```python\n",
    "labs_href=\n",
    "[\n",
    "    [ [href,href,href,.....],類別1 ],\n",
    "    [ [href,href,href,.....],類別2 ],\n",
    "    [ [href,href,href,.....],類別3 ],\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payload(item):\n",
    "    payload={\n",
    "        'id': '1056893641102242',\n",
    "    'ev': 'Microdata',\n",
    "    'dl': item[0],\n",
    "    'rl': 'https://www.cna.com.tw/',\n",
    "    'if': 'false',\n",
    "    'ts': '1541087113735',\n",
    "    'cd[Schema.org]': [],\n",
    "    #'cd[OpenGraph]': {\"og:url\":\"https://www.cna.com.tw/list/ahel.aspx\",\"og:title\":\"生活 | 中央社 CNA\",\"og:description\":\"想知道統一發票、大樂透、威力彩開獎號碼、各種醫療保健、交通氣象、颱風動態等生活資訊，及升學考試、教育政策、課綱改革等文教訊息，通通都在中央社生活新聞。\",\"og:image:height\":\"200\",\"og:image:width\":\"200\",\"og:image\":\"https://img5.cna.com.tw/www/images/pic_fb.jpg\",\"og:type\":\"article\",\"article:author\":\"https://www.facebook.com/cnanewstaiwan\",\"article:publisher\":\"https://www.facebook.com/cnanewstaiwan\"}\n",
    "    #'cd[Meta]': {\"title\":\"\\n\\t生活 | 中央社 CNA\\n\",\"meta:description\":\"想知道統一發票、大樂透、威力彩開獎號碼、各種醫療保健、交通氣象、颱風動態等生活資訊，及升學考試、教育政策、課綱改革等文教訊息，通通都在中央社生活新聞。\",\"meta:keywords\":\"News, 新聞, 即時新聞, 中央社\"}\n",
    "    'cd[DataLayer]': [],\n",
    "    'cd[JSON-LD]': [],\n",
    "    'sw': '1280',\n",
    "    'sh': '5000',#'720',\n",
    "    'v': '2.8.30',\n",
    "    'r': 'stable',\n",
    "    'a': 'tmgoogletagmanager',\n",
    "    'ec': '1',\n",
    "    'o': '30',\n",
    "    'fbp': 'fb.2.1541074934894.620990608',\n",
    "    'it': '1541087112886',\n",
    "    'coo': 'false',\n",
    "    'es': 'automatic',\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_href=[]\n",
    "for item in labs:\n",
    "    soups=[]\n",
    "    n=1\n",
    "    while n < 13:\n",
    "        url = 'https://www.cna.com.tw/cna2018api/api/simplelist/categorycode/'+item[0]+'/pageidx/'+str(n)\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text,'html5lib')\n",
    "        body = soup.findAll('body')[0].getText()\n",
    "        PageUrl = re.findall(r'\\\"PageUrl\\\"\\:\\S[^\\,]+',body)\n",
    "        urls = [txt[11:-1] for txt in PageUrl]\n",
    "        soups.extend(urls)\n",
    "        n += 1\n",
    "        if len(urls) < 20:\n",
    "            break\n",
    "    labs_href.append([soups,item[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 對labs_href中每一個href進行爬蟲抓出新聞標題與內文，並將標題、內文、類別整理成newslist\n",
    "```python\n",
    "newslist=\n",
    "[\n",
    "    [ [標題,內文],類別 ]\n",
    "    [ [標題,內文],類別 ]\n",
    "    [ [標題,內文],類別 ]\n",
    "    [ [標題,內文],類別 ]\n",
    "    ...\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('社會', 20),('社會', 40),('社會', 60),('社會', 80),('社會', 100),('地方', 20),('地方', 40),('地方', 60),('地方', 80),('地方', 100),('兩岸', 20),('兩岸', 40),('兩岸', 60),('兩岸', 80),('兩岸', 100),('運動', 20),('運動', 40),('運動', 60),('運動', 80),('運動', 100),('娛樂', 20),('娛樂', 40),('娛樂', 60),('娛樂', 80),('娛樂', 100),('證券', 20),('證券', 40),('證券', 60),('證券', 80),('證券', 100),('國際', 20),('國際', 40),('國際', 60),('國際', 80),('國際', 100),('生活', 20),('生活', 40),('生活', 60),('生活', 80),('生活', 100),('文化', 20),('文化', 40),('文化', 60),('文化', 80),('文化', 100),('產經', 20),('產經', 40),('產經', 60),('產經', 80),('產經', 100),('科技', 20),('科技', 40),('科技', 60),('科技', 80),('科技', 100),('政治', 20),('政治', 40),('政治', 60),('政治', 80),('政治', 100),"
     ]
    }
   ],
   "source": [
    "news={}\n",
    "newslist=[]\n",
    "for lab in labs_href:\n",
    "    lab_name = lab[1]\n",
    "    lab_urls = lab[0]\n",
    "    n = 0\n",
    "    for url in lab_urls:\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text,'html5lib')\n",
    "        txt = \"\"\n",
    "        try:\n",
    "            title = soup.find_all('div',{'class':'centralContent'})[0].find_all('h1')[0].getText()\n",
    "            for s in soup.find_all('div',{'class':'paragraph'})[0].findAll('p'):\n",
    "                txt=txt+s.getText()\n",
    "        except:IndexError\n",
    "        research = re.search(r'[\\）\\)\\：\\:\\》](.+[\\。\\!\\?\\s])+',txt)\n",
    "        if research == None or len(research.group())<2:\n",
    "            print(txt)\n",
    "        else:\n",
    "            txt = re.search(r'[\\）\\)\\：\\:\\》](.+[\\。\\!\\?\\s])+',txt).group()[1:]\n",
    "\n",
    "        page = [[title,txt],lab_name]\n",
    "        newslist.append(page)\n",
    "        news[(title,txt)]=lab_name\n",
    "        n += 1\n",
    "        if n % 20 == 0:\n",
    "            print((lab_name,n),end=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 找出所有類別的高頻字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將newslist各種分類的標題與內文整合成title_tags , txt_tags\n",
    "```python\n",
    "title_tags=\n",
    "{\n",
    "    類別 : 全標題文字\n",
    "}\n",
    "\n",
    "txt_tags=\n",
    "{\n",
    "    類別 : 全內文文字\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "with open('../30/txt/stopwords_ch.txt', 'r', encoding='UTF-8') as file:\n",
    "    for data in file.readlines():\n",
    "        data = data.strip()\n",
    "        stopwords.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_tags ={}\n",
    "prelab = ''\n",
    "books = list(news.items())\n",
    "for book in books:\n",
    "    for lab in ['生活','運動','政治','科技','國際','兩岸','產經','社會','地方','娛樂','證券','文化']:\n",
    "        if book[1]==lab:\n",
    "            if prelab != lab:\n",
    "                txt=''\n",
    "            txt = txt+book[0][1]\n",
    "            lab_tags[book[1]] = txt\n",
    "            prelab = lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 做一個過濾字池，當無訊息價值的字沒出現在停止詞字典，又不想修改字典就另外做一個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterwords=[\n",
    "    '年','月','中央社','記者','編輯','譯者','日電',\n",
    "    '\\n',' ','「','」'\n",
    "    ,'今天','說','表示','年','月','日','後','前','時','應','中','今年'\n",
    "    ,'兩','下','人','最','次','再','先','盡量'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "titles = []\n",
    "for news in newslist:\n",
    "    articles.append((news[0][1],news[1]))\n",
    "    titles.append((news[0][0],news[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "\n",
    "n = 0\n",
    "while n < len(titles):\n",
    "    head_dic = {}\n",
    "    txt_dic = {}\n",
    "    head = titles[n]\n",
    "    txt = articles[n]\n",
    "    \n",
    "    head_dic[head[0]] = head[1]\n",
    "    txt_dic[txt[0]] = txt[1]\n",
    "    books.append([head_dic,txt_dic])\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(books=books):\n",
    "\n",
    "    prelab = ''\n",
    "    tags = {}\n",
    "    for book in books:\n",
    "        head_dic = book[0]\n",
    "        txt_dic = book[1]\n",
    "        n=0\n",
    "        for dic in [head_dic,txt_dic]:\n",
    "\n",
    "            for lab in ['生活','運動','政治','科技','國際','兩岸','產經','社會','地方','娛樂','證券','文化']:\n",
    "                if list(dic.values())[0]==lab:\n",
    "                    if prelab != lab:\n",
    "                        txt=''\n",
    "                    txt += list(dic.keys())[0]\n",
    "                    tags[list(dic.values())[0]] = txt\n",
    "                    prelab = lab\n",
    "            if n == 0:\n",
    "                title_tags = tags\n",
    "            elif n == 1:\n",
    "                txt_tags = tags\n",
    "            n += 1\n",
    "    return title_tags,txt_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags,txt_tags = get_tags(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title_tags['運動']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將title_tags，txt_tags中全文字斷詞為tokens，統計tokens找出高頻字整理為lab_fwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fwords(tags={},top=300):\n",
    "\n",
    "\n",
    "    lab_fwords={}\n",
    "#    lab_Ftwowords = {}\n",
    "    for labname in tags.keys():\n",
    "        print(labname,end=',')\n",
    "        lab_words = tags[labname]\n",
    "        seg_list = jieba.cut(lab_words, cut_all=False)\n",
    "        txt = \",\".join(seg_list)\n",
    "        tokens = txt.split(',')\n",
    "        tokens = list(filter(lambda a: a not in stopwords and a not in filterwords and a.isalpha() == True,tokens))#and len(a)>1, tokens))\n",
    "        lab_alltwowords = list(nltk.bigrams(tokens))\n",
    "        two_tokens =  [(i[0]+i[1]) for i in lab_alltwowords]\n",
    "        tokens.extend(two_tokens)\n",
    "        lab_fwords[labname] =  [i[0] for i in nltk.FreqDist(tokens).most_common(top)]\n",
    "    return lab_fwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\gr11n\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "社會,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.098 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地方,兩岸,運動,娛樂,證券,國際,生活,文化,產經,科技,政治,"
     ]
    }
   ],
   "source": [
    "txt_fwords3 =get_fwords(txt_tags)\n",
    "#txt_fwords3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義remove_mix(lab_fwords)，刪除交集在各類別中的高頻字，加強高頻字辨識度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除交集的高頻字\n",
    "def remove_mix(t):\n",
    "    lab_fwords = t\n",
    "\n",
    "    dic3={}\n",
    "    for lab in lab_fwords.keys():   \n",
    "        word_content2 = lab_fwords[lab].copy()\n",
    "        for lab2 in lab_fwords.keys():        \n",
    "            if lab != lab2:\n",
    "                word_content = [w for w in lab_fwords[lab] if (w in [a for a in lab_fwords[lab2]])]\n",
    "                for w in word_content:\n",
    "                    try:\n",
    "                        word_content2.remove(w)                  \n",
    "                    except ValueError:\n",
    "                        pass\n",
    "        dic3[lab] = word_content2        \n",
    "    return dic3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "社會,地方,兩岸,運動,娛樂,證券,國際,生活,文化,產經,科技,政治,社會,地方,兩岸,運動,娛樂,證券,國際,生活,文化,產經,科技,政治,"
     ]
    }
   ],
   "source": [
    "title_fwords = remove_mix(get_fwords(title_tags,700))\n",
    "txt_fwords = remove_mix(get_fwords(txt_tags,800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 抽取特徵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義pos_fwords()，找出文章中出現在各類別中的高頻字\n",
    "\n",
    "pos_fwords(article,title=False,top=200)\n",
    "- article = str，目標文本\n",
    "- title = bool，文本是否是標題\n",
    "- top = int，選取前幾名高頻字進行特徵抽取\n",
    "```\n",
    "output = {\n",
    "    word1 : True,\n",
    "    word2 : True,\n",
    "    word3 : True,\n",
    "    ...\n",
    "    }\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_fwords(article,title=False,top=200):\n",
    "    if title==True:\n",
    "        fwords = title_fwords\n",
    "    elif title==False:\n",
    "        fwords = txt_fwords\n",
    "    else:\n",
    "        print('title=True or False')\n",
    "    features={}\n",
    "    t = ''\n",
    "    seg_list = jieba.cut_for_search(article)\n",
    "    t = \",\".join(seg_list)\n",
    "    article_tokens = t.split(',')\n",
    "    article_tokens = [w for w in article_tokens if w.isalpha()]\n",
    "\n",
    "    filt_tokens = list(filter(lambda a: a not in stopwords and a not in filterwords , article_tokens))\n",
    "\n",
    "    toks = [w[0] for w in nltk.FreqDist(filt_tokens).most_common(top)]\n",
    "\n",
    "\n",
    "    if len([i for i in toks if i in [w for li in fwords.values() for w in li]])==0 :\n",
    "        for i in toks[:int(top)]:\n",
    "            features[i] = True\n",
    "    else:\n",
    "        bigcontent=[]\n",
    "        kk=''\n",
    "        for k in fwords.keys():\n",
    "            content=[]\n",
    "            for tok in toks:\n",
    "                if tok in fwords[k] :\n",
    "                    content.append(tok)\n",
    "                if len(bigcontent) < len(content):\n",
    "                    bigcontent = content.copy()\n",
    "                    kk = k\n",
    "\n",
    "        for tok in bigcontent:  \n",
    "            features[tok] = True\n",
    "    if len(list(features.keys())) < 3:\n",
    "        features={}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_fwords(test_txt,title=False,top=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義特徵抽取器get_features()，抽出文章特徵\n",
    "get_features(dic={},title=False,top=200)\n",
    "\n",
    "- dic = dic，{ 字串 : 類別 }\n",
    "- title = bool，字串是否為標題\n",
    "- top = int，選取前幾名高頻字進行特徵抽取\n",
    "\n",
    "```\n",
    "output = [ pos_fwords() , 類別 ]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dic={},title=False,top=200):\n",
    "    n = 0\n",
    "#    head_dic = book[0]\n",
    "    article = list(dic.keys())[0]\n",
    "    labname = list(dic.values())[0]\n",
    "    features = pos_fwords(article,title,top)\n",
    "\n",
    "    if len(list(features.keys())) < 2:\n",
    "        feature_item = [{},labname]\n",
    "    else:\n",
    "        feature_item = [features,labname]\n",
    "\n",
    "    return feature_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義get_featuresets()，將分類好的文本集books由特徵抽取器get_features()做成機器學習接受格式的特徵集\n",
    "```\n",
    "output=[\n",
    "    [ pos_fwords(文本1) , 類別 ],\n",
    "    [ pos_fwords(文本2) , 類別 ],\n",
    "    [ pos_fwords(文本3) , 類別 ],\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featuresets(books=books,top=200):\n",
    "    title_featuresets = []\n",
    "    txt_featuresets = []\n",
    "    for book in books:\n",
    "\n",
    "        title_featureset = get_features(book[0],title=True,top=top)\n",
    "        title_featuresets.append(title_featureset)\n",
    "        \n",
    "        txt_featureset = get_features(book[1],title=False,top=top)\n",
    "        txt_featuresets.append(txt_featureset)\n",
    "    return title_featuresets, txt_featuresets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 使用特徵集進行機器學習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作標題與內文2份特徵集，分別以7:3的比例作為訓練集和測試集，最後使用貝氏分類器製作模型並回傳精確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_featuresets, txt_featuresets = get_featuresets(top=170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2777777777777778"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_featuresets = title_featuresets.copy()\n",
    "size = int(len(title_featuresets)*0.7)\n",
    "train_set, test_set = title_featuresets[:size], title_featuresets[size:]\n",
    "title_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(title_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8611111111111112"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_featuresets = txt_featuresets.copy()\n",
    "size = int(len(txt_featuresets)*0.7)\n",
    "train_set, test_set = txt_featuresets[:size], txt_featuresets[size:]\n",
    "txt_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(txt_classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用txt_classifier.classify_many()觀察測試結果，並與測試集進行比對"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_comanswer = title_classifier.classify_many([i[0] for i in title_featuresets])\n",
    "txt_comanswer = txt_classifier.classify_many([i[0] for i in txt_featuresets])\n",
    "\n",
    "title_answer = [i[1] for i in title_featuresets]\n",
    "txt_answer = [i[1] for i in txt_featuresets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_correct = ['F' if w != title_answer[title_comanswer.index(w)] else 'T' for w in title_comanswer ]\n",
    "txt_correct = ['F' if w != txt_answer[txt_comanswer.index(w)] else 'T' for w in txt_comanswer ]\n",
    "df = pd.DataFrame([txt_comanswer,txt_answer,txt_correct,title_comanswer,title_answer,title_correct],index=['txt_comanswer','txt_answer','txt_correct','title_comanswer','title_answer','title_correct']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt_comanswer</th>\n",
       "      <th>txt_answer</th>\n",
       "      <th>txt_correct</th>\n",
       "      <th>title_comanswer</th>\n",
       "      <th>title_answer</th>\n",
       "      <th>title_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>運動</td>\n",
       "      <td>運動</td>\n",
       "      <td>T</td>\n",
       "      <td>運動</td>\n",
       "      <td>運動</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>娛樂</td>\n",
       "      <td>文化</td>\n",
       "      <td>F</td>\n",
       "      <td>證券</td>\n",
       "      <td>文化</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>國際</td>\n",
       "      <td>國際</td>\n",
       "      <td>T</td>\n",
       "      <td>國際</td>\n",
       "      <td>國際</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>國際</td>\n",
       "      <td>國際</td>\n",
       "      <td>T</td>\n",
       "      <td>證券</td>\n",
       "      <td>國際</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>社會</td>\n",
       "      <td>社會</td>\n",
       "      <td>T</td>\n",
       "      <td>證券</td>\n",
       "      <td>社會</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>證券</td>\n",
       "      <td>證券</td>\n",
       "      <td>T</td>\n",
       "      <td>證券</td>\n",
       "      <td>證券</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>運動</td>\n",
       "      <td>運動</td>\n",
       "      <td>T</td>\n",
       "      <td>運動</td>\n",
       "      <td>運動</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>文化</td>\n",
       "      <td>文化</td>\n",
       "      <td>T</td>\n",
       "      <td>文化</td>\n",
       "      <td>文化</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>證券</td>\n",
       "      <td>證券</td>\n",
       "      <td>T</td>\n",
       "      <td>證券</td>\n",
       "      <td>證券</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>證券</td>\n",
       "      <td>產經</td>\n",
       "      <td>T</td>\n",
       "      <td>證券</td>\n",
       "      <td>產經</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  txt_comanswer txt_answer txt_correct title_comanswer title_answer  \\\n",
       "0            運動         運動           T              運動           運動   \n",
       "1            娛樂         文化           F              證券           文化   \n",
       "2            國際         國際           T              國際           國際   \n",
       "3            國際         國際           T              證券           國際   \n",
       "4            社會         社會           T              證券           社會   \n",
       "5            證券         證券           T              證券           證券   \n",
       "6            運動         運動           T              運動           運動   \n",
       "7            文化         文化           T              文化           文化   \n",
       "8            證券         證券           T              證券           證券   \n",
       "9            證券         產經           T              證券           產經   \n",
       "\n",
       "  title_correct  \n",
       "0             T  \n",
       "1             F  \n",
       "2             T  \n",
       "3             F  \n",
       "4             F  \n",
       "5             F  \n",
       "6             T  \n",
       "7             T  \n",
       "8             F  \n",
       "9             F  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 使用訓練結果再做一次機器學習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 由於依照標題辨識類別的精確度太低，另外加入新的特徵\n",
    "### - 定義get_top()統計文本高頻字出現最多次的類別\n",
    "```\n",
    "get_top(article,top,get=20)=\n",
    "- article = str，目標文本\n",
    "- top = int，高頻字數量\n",
    "- get = int，統計前幾名高頻字類別\n",
    "\n",
    "output = 類別\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_top(article,top,get=20):\n",
    "#    print(article)\n",
    "    features={}\n",
    "    t = ''\n",
    "    seg_list = jieba.cut_for_search(article)\n",
    "    t = \",\".join(seg_list)\n",
    "    article_tokens = t.split(',')\n",
    "    article_tokens = [w for w in article_tokens if w.isalpha()]\n",
    "\n",
    "    filt_tokens = list(filter(lambda a: a not in stopwords and a not in filterwords , article_tokens))\n",
    "    toks = [w for w in nltk.FreqDist(filt_tokens).most_common(top)]\n",
    "    txt_comanwer = txt_classifier.classify(pos_fwords(article,title=False))\n",
    "\n",
    "    try:\n",
    "        selectwords = ([t for t in toks if t[0] in [w for li in txt_fwords.values() and t[1]>=1 for w in li]])\n",
    "        klist=[]\n",
    "        for w in selectwords[:get]:  \n",
    "            for k in txt_fwords.keys():\n",
    "                if w[0] in [s for s in txt_fwords[k]] :\n",
    "#                    print([w[0],k])\n",
    "                    klist.append(k)\n",
    "#        print(\"\")\n",
    "        one = Counter(klist).most_common(1)[0][0]\n",
    "        two = Counter(klist).most_common(2)[1][0]\n",
    "        count1 = Counter(klist).most_common(1)[0][1]\n",
    "        count2 = Counter(klist).most_common(2)[1][1]\n",
    "        X = int((count1+count2)/6)\n",
    "        if X < 1:\n",
    "            X == 1\n",
    "        \n",
    "        if Counter(klist).most_common(1)[0][1] - Counter(klist).most_common(2)[1][1] <=X and (one == txt_comanwer or two == txt_comanwer):\n",
    "            features['top'] = txt_comanwer\n",
    "        else:\n",
    "            features['top'] = Counter(klist).most_common(1)[0][0]\n",
    "    except:\n",
    "        try:\n",
    "            selectwords = ([t for t in toks if t[0] in [w for li in txt_fwords.values()  for w in li]])\n",
    "            klist=[]\n",
    "            for w in selectwords[:get]:  \n",
    "                for k in txt_fwords.keys():\n",
    "                    if w[0] in [s for s in txt_fwords[k]]  and w[1]>=1 :\n",
    "#                        print([w[0],k])\n",
    "                        klist.append(k)\n",
    "#            print(\"\")\n",
    "            one = Counter(klist).most_common(1)[0][0]\n",
    "            two = Counter(klist).most_common(2)[1][0]\n",
    "            count1 = Counter(klist).most_common(1)[0][1]\n",
    "            count2 = Counter(klist).most_common(2)[1][1]\n",
    "            X = int((count1+count2)/6)\n",
    "            if X < 1:\n",
    "                X == 1\n",
    "#            print(Counter(klist).most_common(5))\n",
    "            if Counter(klist).most_common(1)[0][1] - Counter(klist).most_common(2)[1][1] <=X and (one == txt_comanwer or two == txt_comanwer):\n",
    "                features['top'] = txt_comanwer\n",
    "            else:\n",
    "                features['top'] = Counter(klist).most_common(1)[0][0]\n",
    "        except IndexError:\n",
    "            try:\n",
    "                selectwords = ([t for t in toks if t[0] in [w for li in txt_fwords3.values() for w in li]])\n",
    "                klist=[]\n",
    "                for w in selectwords[:get]:  \n",
    "                    for k in txt_fwords3.keys():\n",
    "                        if w[0] in [s for s in txt_fwords3[k]] and w[1]>=3 :\n",
    "                            klist.append(k)\n",
    "#                        print([w[0],k])\n",
    "#                print(\"\")\n",
    "                one = Counter(klist).most_common(1)[0][0]\n",
    "                two = Counter(klist).most_common(2)[1][0]\n",
    "                count1 = Counter(klist).most_common(1)[0][1]\n",
    "                count2 = Counter(klist).most_common(2)[1][1]\n",
    "                X = int((count1+count2)/6)\n",
    "                if X < 1:\n",
    "                    X == 1\n",
    "                if Counter(klist).most_common(1)[0][1] - Counter(klist).most_common(2)[1][1] <=X and (one == txt_comanwer or two == txt_comanwer):\n",
    "                    features['top'] = txt_comanwer\n",
    "                else:\n",
    "                    features['top'] = Counter(klist).most_common(1)[0][0]\n",
    "            except :\n",
    "                selectwords = ([t for t in toks if t[0] in [w for li in txt_fwords3.values() for w in li]])\n",
    "                klist=[]\n",
    "                for w in selectwords[:get]:  \n",
    "                    for k in txt_fwords3.keys():\n",
    "                        if w[0] in [s for s in txt_fwords3[k]]  :\n",
    "                            klist.append(k)\n",
    " #                       print([w[0],k])\n",
    " #               print(\"\")\n",
    "                one = Counter(klist).most_common(1)[0][0]\n",
    "                two = Counter(klist).most_common(2)[1][0]\n",
    "                count1 = Counter(klist).most_common(1)[0][1]\n",
    "                count2 = Counter(klist).most_common(2)[1][1]\n",
    "                X = int((count1+count2)/6)\n",
    "                if X < 1:\n",
    "                    X == 1\n",
    "                if Counter(klist).most_common(1)[0][1] - Counter(klist).most_common(2)[1][1] <=X and (one == txt_comanwer or two == txt_comanwer):\n",
    "                    features['top'] = txt_comanwer\n",
    "                else:\n",
    "                    features['top'] = Counter(klist).most_common(1)[0][0]\n",
    "    \n",
    "#    print(['one='+one,'two='+two,'txt_comanwer='+txt_comanwer])\n",
    "#    print('txt_comanwer='+txt_comanwer)\n",
    "#    print(features.values())\n",
    "#    print(\"\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_top = [list(get_top(txt,175,175).values())[0] for txt in [list(book[1].keys())[0] for book in books]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作新的特徵集dataset\n",
    "```\n",
    "dataset=[\n",
    "    ({'title_comanswer': 預測類別, 'txt_comanswer': 預測類別, 'txt_top': 預測類別}, 正確類別),\n",
    "    ({'title_comanswer': 預測類別, 'txt_comanswer': 預測類別, 'txt_top': 預測類別}, 正確類別),\n",
    "    ({'title_comanswer': 預測類別, 'txt_comanswer': 預測類別, 'txt_top': 預測類別}, 正確類別),\n",
    "    ...   \n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Sklearn做貝氏模型與SVM模型，並得出精確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame([txt_comanswer,txt_top,txt_answer,txt_correct,title_comanswer,title_answer,title_correct],index=['txt_comanswer','txt_top','txt_answer','txt_correct','title_comanswer','title_answer','title_correct']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "answer = \"\"\n",
    "for i,v in df2.iterrows():\n",
    "    answer_dict={}\n",
    "#    print(v)\n",
    "    answer_dict['txt_comanswer']=v[0]\n",
    "    answer_dict['txt_top']=v[1]\n",
    "    answer_dict['title_comanswer']=v[4]\n",
    "\n",
    "    answer = v[1]\n",
    "    dataset.append((answer_dict,answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(dataset)*0.7)\n",
    "train_data, test_data = dataset[:size], dataset[size:]\n",
    "test_data2 = [i[0] for i in test_data]\n",
    "\n",
    "classif0 = SklearnClassifier(BernoulliNB()).train(train_data)\n",
    "classif = SklearnClassifier(SVC(), sparse=False).train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[({'title_comanswer': '證券', 'txt_comanswer': '國際', 'txt_top': '國際'}, '國際'),\n",
       "  '國際'],\n",
       " [({'title_comanswer': '證券', 'txt_comanswer': '地方', 'txt_top': '地方'}, '地方'),\n",
       "  '地方'],\n",
       " [({'title_comanswer': '兩岸', 'txt_comanswer': '兩岸', 'txt_top': '兩岸'}, '兩岸'),\n",
       "  '兩岸'],\n",
       " [({'title_comanswer': '證券', 'txt_comanswer': '文化', 'txt_top': '文化'}, '文化'),\n",
       "  '文化'],\n",
       " [({'title_comanswer': '運動', 'txt_comanswer': '運動', 'txt_top': '運動'}, '運動'),\n",
       "  '運動']]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = []\n",
    "n=0\n",
    "for f in dataset[size:] :\n",
    "    ch.append([f,classif.classify_many(test_data2)[n]])\n",
    "    n += 1\n",
    "ch[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將測試集帶入模型回傳預測類別，將預測類別與測試集正確類別比對，回傳正確率並列出錯誤文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "back = classif.classify_many(test_data2)\n",
    "n = 0\n",
    "p=0\n",
    "while n < len(back):\n",
    "    if back[n] ==  [i[1] for i in test_data][n]:\n",
    "        p += 1\n",
    "    \n",
    "    else:\n",
    "        print(back[n],' ',[i[1] for i in test_data][n],' ',[i[0] for i in test_data][n])\n",
    "    n += 1\n",
    "print(p)\n",
    "print(p/len(back))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 定義get_lab()將兩次機器學習模型放進去，回傳預測結果\n",
    "\n",
    "### get_lab( 標題 , 內文 ) = 預測類別\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lab(title='',txt='',top=200,get=30):\n",
    "    f = pos_fwords(txt,False,top)\n",
    "    t =  pos_fwords(title,True,top)\n",
    "    txttop = list(get_top(txt,top,get).values())[0]\n",
    "\n",
    "    if len(t.keys()) < 2:\n",
    "        final = classif.classify_many({'txt_comanswer': txt_classifier.classify(f),'txt_top':txttop})\n",
    "        final_title = \"Na\"\n",
    "    elif title_classifier.classify(t) == txt_classifier.classify(f) and txt_classifier.classify(f)==top and top == title_classifier.classify(t):\n",
    "        final = txt_classifier.classify(f)\n",
    "        final_title = title_classifier.classify(t)\n",
    "    else:\n",
    "        final = classif.classify_many({'title_comanswer': title_classifier.classify(t), 'txt_comanswer': txt_classifier.classify(f),'txt_top':txttop})\n",
    "        final_title = title_classifier.classify(t)\n",
    "    print(f)\n",
    "    print([final_title,txt_classifier.classify(f),txttop],'p')\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 測試其他新聞網站的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title=\"\"\"\n",
    "昨探視江丙坤 柯Ｐ：學生叫他去看要不要撤掉管子\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt=\"\"\"\n",
    "前海基會董事長江丙坤昨晚不幸病逝馬偕醫院，享壽86歲，過世前台北市長柯文哲有去探望他，媒體今（11）日問柯文哲，為何會去看江丙坤？柯文哲表示，馬偕醫院心臟外科醫師是他在台大的學生，遇到這種V.I.P.，他們比較不想自己做決定，所以叫老師去看，看到底要不要撤掉管子。\n",
    "\n",
    "\n",
    "\n",
    "柯文哲說，自己曾經是重症醫學專家，因為比較年輕的醫師要去跟家屬說，這一定不會活，要撤管，當然有壓力，所以就由他去處理。媒體追問，「所以是你跟家屬說拔管嗎？」柯並未回應。\n",
    "\n",
    "\n",
    "\n",
    "此外，台北市長驗票結果今天將出爐，國民黨候選人丁守中仍輸柯文哲3千多票，媒體今問柯，是否擔心丁守中提選舉無效訴訟，對此，柯文哲表示，他會去問律師，這要花多少錢，募的錢都快不夠了。\n",
    "\n",
    "\n",
    "\n",
    "至於是否會私底下找丁守中，勸丁不要提選舉無效訴訟，柯則說，這不容易，這一次驗票 單單民政局就花了一千一百萬元，因為二備金他要簽，不曉得司法系統要花多少錢，然後競選總部的律師也要花錢，他猜丁守中花更多。\n",
    " \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "['Na', '證券', '政治'] p\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['政治']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lab(title=test_title,txt=test_txt,top=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 8. 用類神經網路建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把高頻字的特徵提取器輸出修改為符合MLPClassifier接受格式，並將特徵集分為訓練集和測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top2(article,top,get=20):\n",
    "#    print(article)\n",
    "    features={}\n",
    "    t = ''\n",
    "    seg_list = jieba.cut_for_search(article)\n",
    "    t = \",\".join(seg_list)\n",
    "    article_tokens = t.split(',')\n",
    "    article_tokens = [w for w in article_tokens if w.isalpha()]\n",
    "    filt_tokens = list(filter(lambda a: a not in stopwords and a not in filterwords , article_tokens))\n",
    "    toks = [w for w in nltk.FreqDist(filt_tokens).most_common(top)]\n",
    "    txt_comanwer = txt_classifier.classify(pos_fwords(article,title=False))\n",
    "    selectwords = ([t for t in toks if t[0] in [w for li in txt_fwords.values() for w in li]])\n",
    "    klist=[]\n",
    "    \n",
    "    for k in txt_fwords.keys():\n",
    "        kwc = 0\n",
    "        for w in selectwords[:get]:        \n",
    "            if w[0] in [s for s in txt_fwords[k]]  :\n",
    "                kwc += 1\n",
    "                klist.append(k)\n",
    "            features[k] = kwc\n",
    "\n",
    "    return list(features.values())[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [i for i in [[list(book[1].keys())[0],list(book[1].values())[0]] for book in books[:800]]]\n",
    "test_set = [i for i in [[list(book[1].keys())[0],list(book[1].values())[0]] for book in books[800:]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [get_top2(i[0],top=250,get=30) for i in train_set]\n",
    "train_labs = [i[1] for i in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = [get_top2(i[0],top=250,get=30) for i in test_set ]\n",
    "test_labs = [i[1] for i in test_set ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用類神經網路，建立模型mlp\n",
    "\n",
    "- 深度3層\n",
    "- 每層30個節點\n",
    "- 訓練2000次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(30, 30, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30), activation='logistic', max_iter = 2000)\n",
    "mlp.fit(train_features,train_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[mlp.predict(i[0]) for i in test_set if i[1]=='文化']\n",
    "Confusion = []\n",
    "for n in range(0,len(list(mlp.predict(test_features)))):\n",
    "    Confusion.append([list(mlp.predict(test_features))[n],test_labs[n]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用模型辨別測試集資料類別，並將預測結果與正確類別比對得出精確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (mlp.predict(test_features) == test_labs)\n",
    "len([i for i in a if i==True])/len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 製作預測結果的混淆矩陣，觀察各類別的一型與二型錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Confusion_dic={}\n",
    "for k in txt_fwords.keys():\n",
    "    Matrix={}\n",
    "    count = []\n",
    "    for kk in txt_fwords.keys():\n",
    "        Matrix[kk] =  [i[0] for i in Confusion if i[1]==k].count(kk)\n",
    "        filt = [i[0] for i in Confusion if i[1]==k]\n",
    "        i = filt.count(kk)\n",
    "\n",
    "        count.append(i)\n",
    "    Confusion_dic[k] = count\n",
    "#print(Confusion_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>社會</th>\n",
       "      <th>地方</th>\n",
       "      <th>兩岸</th>\n",
       "      <th>運動</th>\n",
       "      <th>娛樂</th>\n",
       "      <th>證券</th>\n",
       "      <th>國際</th>\n",
       "      <th>生活</th>\n",
       "      <th>文化</th>\n",
       "      <th>產經</th>\n",
       "      <th>科技</th>\n",
       "      <th>政治</th>\n",
       "      <th>總計</th>\n",
       "      <th>錯誤</th>\n",
       "      <th>精確度</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>output</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>社會</th>\n",
       "      <td>31.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>地方</th>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>兩岸</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>運動</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>娛樂</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>證券</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>國際</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>生活</th>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>文化</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>產經</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>科技</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>政治</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>總計</th>\n",
       "      <td>33.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>34.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>400.00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>生產者精度</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           社會     地方     兩岸     運動     娛樂     證券     國際     生活     文化     產經  \\\n",
       "output                                                                         \n",
       "社會      31.00   1.00   1.00   1.00   0.00   0.00   0.00   1.00   0.00   0.00   \n",
       "地方       1.00  30.00   0.00   0.00   1.00   1.00   0.00   3.00   0.00   2.00   \n",
       "兩岸       0.00   0.00  31.00   1.00   0.00   0.00   1.00   0.00   0.00   0.00   \n",
       "運動       0.00   0.00   0.00  31.00   0.00   0.00   0.00   1.00   0.00   0.00   \n",
       "娛樂       0.00   0.00   1.00   0.00  33.00   2.00   0.00   0.00   0.00   0.00   \n",
       "證券       0.00   0.00   0.00   0.00   0.00  20.00   0.00   0.00   0.00   2.00   \n",
       "國際       0.00   1.00   1.00   1.00   1.00   0.00  30.00   0.00   1.00   1.00   \n",
       "生活       1.00   3.00   1.00   2.00   0.00   0.00   0.00  21.00   1.00   0.00   \n",
       "文化       0.00   2.00   2.00   0.00   0.00   0.00   0.00   0.00  32.00   0.00   \n",
       "產經       0.00   1.00   2.00   0.00   0.00   2.00   1.00   0.00   0.00  29.00   \n",
       "科技       0.00   0.00   1.00   1.00   0.00   1.00   1.00   0.00   0.00   0.00   \n",
       "政治       0.00   2.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   1.00   \n",
       "總計      33.00  40.00  40.00  37.00  35.00  26.00  33.00  26.00  34.00  35.00   \n",
       "生產者精度    0.94   0.75   0.78   0.84   0.94   0.77   0.91   0.81   0.94   0.83   \n",
       "\n",
       "           科技     政治      總計    錯誤   精確度  \n",
       "output                                    \n",
       "社會       0.00   0.00   35.00   4.0  0.89  \n",
       "地方       0.00   0.00   38.00   8.0  0.79  \n",
       "兩岸       0.00   4.00   37.00   6.0  0.84  \n",
       "運動       0.00   0.00   32.00   1.0  0.97  \n",
       "娛樂       0.00   0.00   36.00   3.0  0.92  \n",
       "證券       1.00   0.00   23.00   3.0  0.87  \n",
       "國際       0.00   0.00   36.00   6.0  0.83  \n",
       "生活       3.00   2.00   34.00  13.0  0.62  \n",
       "文化       1.00   0.00   37.00   5.0  0.86  \n",
       "產經       1.00   0.00   36.00   7.0  0.81  \n",
       "科技      23.00   1.00   28.00   5.0  0.82  \n",
       "政治       0.00  25.00   28.00   3.0  0.89  \n",
       "總計      29.00  32.00  400.00  64.0  0.84  \n",
       "生產者精度    0.79   0.78    0.84   NaN   NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Confusion_Matrix = pd.DataFrame(list(Confusion_dic.values()),index=txt_fwords.keys(),columns=txt_fwords.keys())\n",
    "Confusion_Matrix.reset_index(inplace=True)\n",
    "Confusion_Matrix.rename(columns={'index':'output'},inplace=True)\n",
    "Confusion_Matrix.set_index('output',inplace=True)\n",
    "Confusion_Matrix.loc['總計']=Confusion_Matrix.iloc[:,:].sum()\n",
    "\n",
    "Confusion_Matrix['總計'] = [d.sum() for i,d in Confusion_Matrix.iterrows()]\n",
    "Confusion_Matrix['錯誤'] = [d[-1]-d[i] for i,d in Confusion_Matrix.iterrows() ]\n",
    "Confusion_Matrix.loc['錯誤']=0\n",
    "Confusion_Matrix.loc['錯誤'] = [i[1][-2]-Confusion_Matrix.loc[i[0],i[0]] for i in Confusion_Matrix.iteritems()]\n",
    "\n",
    "Confusion_Matrix.loc['總計']=Confusion_Matrix.iloc[:-2,:].sum()\n",
    "Confusion_Matrix['總計'] = [d[:-2].sum() for i,d in Confusion_Matrix.iterrows()]\n",
    "Confusion_Matrix.loc['生產者精度']=0\n",
    "Confusion_Matrix.loc['生產者精度']=[round(1-(i[1][-2]/i[1][-3]),2) if i[1][-3] != 0 else 0 for i in Confusion_Matrix.iteritems() ]\n",
    "Confusion_Matrix['精確度'] = 1-round((Confusion_Matrix['錯誤']/Confusion_Matrix['總計']),2)\n",
    "Confusion_Matrix.drop(index='錯誤',inplace=True)\n",
    "Confusion_Matrix.iloc[-1,-2:]=None\n",
    "Confusion_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 使用其他網站的新聞文章進行測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news=\"\"\"\n",
    "電腦品牌廠華碩執行長沈振來今天宣布辭職，將投資創捷前瞻公司並出任董事長，協助華碩投入AIOT領域；華碩新設共同執行長由許先越、胡書賓擔任，明年1月1日生效。\n",
    "\n",
    "\n",
    "華碩執行長沈振來。（中央社／資料照片）\n",
    "更多\n",
    "華碩今天下午5時召開董事會通過3項議案，包括設立共同執行長、手機策略轉型計畫、AIOT（人工智慧暨物聯網）新策略事業計畫。董事會後華碩舉行記者會，由董事長施崇棠、沈振來及全球副總裁許先越、全球副總裁胡書賓一同出席。\n",
    "\n",
    "沈振來宣布，為加速公司傳承與轉型，也實踐長久以來個人對職涯規劃的方向，他將辭去華碩執行長職務，往創業方向發展；未來他將協助華碩投入AIOT領域，也將投資創捷前瞻公司並出任董事長。\n",
    "\n",
    "施崇棠表示，他肯定及感謝沈振來過去25年的貢獻，相信以沈振來對產業的熱忱及執行力，將能開創另一個事業榮景，華碩董事會也決議通過將投資創捷前瞻公司30%股份。\n",
    "\n",
    "施崇棠指出，華碩自今年以來，已逐步於內部導入團隊領導機制，許先越及胡書賓同具技術背景及管理能力，合作默契佳，共同管理下可達成「發揮眾智」及「創意擇優」，相信優秀領導團隊的合作將能發揮更完整的團隊能力，達成更遠大目標。\n",
    "\n",
    "華碩手機策略轉型計畫將以專注及價值創造為主軸，專注電競用戶（Gamer）及專家用戶（Power User）。由於手機整體市場及策略的價值轉移，為改善營運體質，華碩將於今年第4季提列一次性費用約新台幣62億元，包含提列存貨損失、權利金資產攤銷及組織調整費用。\n",
    "\n",
    "此外，華碩董事會通過將策略性投入AIOT領域，投入商用及B2B（企業對企業）領域，配合工業電腦產業的進化趨勢，建立人工智慧及移動裝置產品於工業客戶及商用市場的各種應用；另預計於新台幣100億元的額度內，採策略投資及併購方式，快速發展事業及建立團隊，希望於3年內成為新AIOT產業領導廠商。\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lab(test_news='',top=200,get=25):\n",
    "    return mlp.predict(np.array(get_top2(test_news,top=200,get=25)).reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'產經'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_lab(test_news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
